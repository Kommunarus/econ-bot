from langgraph.graph import StateGraph, MessagesState, START, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import AIMessage, SystemMessage, HumanMessage

from collections import OrderedDict
from langgraph.checkpoint.base import BaseCheckpointSaver

from Support_2.rag import retr
from Support_2.web import web_search
from Support_2.agent import create_data_agent

from pydantic import BaseModel, Field
from typing import Literal, List, Dict, Any
import os
from dotenv import load_dotenv
import time
import json


load_dotenv('.env')
MODEL = os.getenv("MODEL")
API_KEY = os.getenv("API_KEY")
API_BASE = os.getenv("API_BASE")
model = ChatOpenAI(model_name=MODEL, openai_api_key=API_KEY, openai_api_base=API_BASE, temperature=0)
data_app, config_data = create_data_agent()



class CustomMessagesState(MessagesState):
    context: str
    route: Literal["RAG", "WEB_SEARCH", "DATA", "DEFAULT"]
    confidence: float
    reasoning: str
    logs: List[Dict[str, Any]] = []

class RouterProfile(BaseModel):
    route: Literal["RAG", "WEB_SEARCH", "DATA", "DEFAULT"] = Field(description="–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞")
    confidence: float = Field(ge=0.0, le=1.0, description="—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞")
    reasoning: str = Field(description="–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∞")

router_model = model.with_structured_output(RouterProfile).bind(temperature=0.1)
ROUTER_SYSTEM_PROMPT = """–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –∫ –æ–¥–Ω–æ–º—É –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:

RAG ‚Äî –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏ (–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è, FAQ, –ø—Ä–æ–¥—É–∫—Ç—ã)
WEB_SEARCH ‚Äî –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
DATA ‚Äî –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö, –≥—Ä–∞—Ñ–∏–∫–∏, CSV‚Äë—Ñ–∞–π–ª—ã
DEFAULT ‚Äî –æ–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤

–ü—Ä–∞–≤–∏–ª–∞:
1. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è DATA: –≥—Ä–∞—Ñ–∏–∫, –¥–∏–∞–≥—Ä–∞–º–º–∞, –ø–æ—Å—Ç—Ä–æ–π, csv, —Ç–∞–±–ª–∏—Ü–∞
2. –î–ª—è WEB_SEARCH: –Ω–æ–≤–æ—Å—Ç–∏, –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ, —Å–µ–π—á–∞—Å, –ø–æ–≥–æ–¥–∞, –≤–Ω–µ—à–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
3. –î–ª—è RAG: "–≤–∞—à", "–≤–∞—à–µ–π –∫–æ–º–ø–∞–Ω–∏–∏", –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, "—É –≤–∞—Å"
4. DEFAULT –¥–ª—è –≤—Å–µ–≥–æ –æ—Å—Ç–∞–ª—å–Ω–æ–≥–æ

–§–æ—Ä–º–∞—Ç: {"route": "RAG|WEB_SEARCH|DATA|DEFAULT", "confidence": 0.0-1.0, "reasoning": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ"}"""

def log_step(logs: List[Dict], step_name: str, input_data=None, output_data=None):
    logs.append({
        "step": step_name,
        "input": input_data,
        "output": output_data
    })

def log_save(logs: List[Dict]):
    log_files = './logs/support.log'
    with open(log_files, 'a') as f:
        for log in logs:
            f.write(json.dumps(log, ensure_ascii=False)+'\n')

def planner(state: CustomMessagesState):
    time_s = time.time()

    query = state["messages"][-1].content

    messages=[
        SystemMessage(content=ROUTER_SYSTEM_PROMPT),
        HumanMessage(content=query)
    ]
    response = router_model.invoke(messages)

    log_step(state["logs"], "planner", input_data=query, output_data=response.model_dump())
    log_step(state["logs"], "planner", input_data='–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', output_data=time.time()-time_s)

    return {
        "route": response.route,
        "confidence": response.confidence,
        "reasoning": response.reasoning
    }


def rag(state: CustomMessagesState):
    time_s = time.time()

    query = state["messages"][-1].content
    context = retr(query)

    log_step(state["logs"], "rag", input_data='–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', output_data=time.time()-time_s)

    return {"context": context}

def Web(state: CustomMessagesState):
    time_s = time.time()

    query = state["messages"][-1].content
    context = web_search(query)

    log_step(state["logs"], "web", input_data='–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', output_data=time.time()-time_s)


    return {"context": context}

def Data(state: CustomMessagesState):
    time_s = time.time()
    max_retries = config_data["configurable"]["max_retries"]
    # —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–¥–∞
    data_app.invoke({
        "messages": state["messages"][-1].content,
        "code": "",
        "result": "",
        "error_message": ""
    }, config_data)

    while True:
        current_state = data_app.get_state(config_data)
        if not current_state.values.get('code'):
            log_step(state["logs"], "data", input_data='–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', output_data=time.time() - time_s)
            return {"context": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö."}

        # print("\n=== –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–¥–∞ ===")
        data_app.invoke(None, config_data)
        final_state = data_app.get_state(config_data)

        if not final_state.values.get('error_message'):
            log_step(state["logs"], "data", input_data='–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', output_data=time.time() - time_s)
            return {
                "context": final_state.values.get('result', ''),
            }

        if final_state.values.get('retry_count', 0) >= max_retries:
            print(f"\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫ ({max_retries}). –ö–æ–¥ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ.")
            log_step(state["logs"], "data", input_data='–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', output_data=time.time() - time_s)
            return {
                "context": f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–¥ –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {final_state.values.get('error_message')}",
            }
        # –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–¥–∞, —Å –¥–æ–±–∞–≤–ª–µ–Ω–æ–π –æ—à–∏–±–∫–æ–π –≤ –ø—Ä–æ–º–ø—Ç
        data_app.invoke(None, config_data)


def generate_response_node(state: CustomMessagesState):
    time_s = time.time()

    query = state["messages"][-1].content
    context = state.get("context", "")
    system_prompt = "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π AI‚Äë–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫—Ä–∞—Ç–∫–æ."

    if state["route"] in ("RAG", "WEB_SEARCH"):
        system_prompt += ' –í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ç–µ–±–µ –¥–∞–Ω–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ'
        answer = f'–í–æ–ø—Ä–æ—Å: {query}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:{context}'
        state["messages"][-1] = answer
    elif state["route"] == "DATA":
        system_prompt += ' –í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ç–µ–±–µ –¥–∞–Ω –≤—ã–≤–æ–¥ –∫–æ–¥–∞'
        answer = f'–í–æ–ø—Ä–æ—Å: {query}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:{context}'
        state["messages"][-1] = answer

    messages = [SystemMessage(content=system_prompt)] + state["messages"]
    response = model.invoke(messages)

    log_step(state["logs"], "llm", input_data='Usage_metadata',
             output_data={key: response.usage_metadata.get(key) for key in ['input_tokens',
                                                                            'output_tokens',
                                                                            'total_tokens']})
    log_step(state["logs"], "llm", input_data='–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', output_data=time.time()-time_s)

    log_save(state["logs"])
    state["logs"] = []


    return {"messages": [response]}



def main_agent():
    graph = StateGraph(CustomMessagesState)



    graph.add_node("planner", planner)
    graph.add_node("RAG", rag)
    graph.add_node("Web", Web)
    graph.add_node("Data", Data)
    graph.add_node("llm", generate_response_node)

    graph.add_edge(START, "planner")
    graph.add_conditional_edges(
        "planner",
        lambda state: state["route"],
        {
            "RAG": "RAG",
            "WEB_SEARCH": "Web",
            "DATA": "Data",
            "DEFAULT": "llm"
        }
    )
    graph.add_edge("RAG", "llm")
    graph.add_edge("Data", "llm")
    graph.add_edge("Web", "llm")
    graph.add_edge("llm", END)

    compiled = graph.compile()

    config = {"configurable": {"thread_id": "llm_calls-1"}}

    return compiled, config

def main(question):
    compiled, config = main_agent()
    response = compiled.invoke({
        'messages': {"role": "human", "content": question},
        'logs': []
         },
        config=config,
    )
    answer = response["messages"][-1].content
    print(f"\nü§ñ –ê–≥–µ–Ω—Ç: (from {response["route"]}) {answer}\n")

    # current_log = json.dumps(response["logs"], indent=4, ensure_ascii=False)
    # print(f'\nLogs: {current_log}')


def interactive_data_analyst():
    compiled, config = main_agent()
    while True:
        user_input = input("üë§ –í—ã: ").strip()

        if user_input.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥']:
            print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break

        if not user_input:
            continue


        try:
            response = compiled.invoke(
                {"messages": [HumanMessage(content=user_input)], "logs": []},
                config
            )

            answer = response["messages"][-1].content
            print(f"\nü§ñ –ê–≥–µ–Ω—Ç (from {response["route"]}): {answer}\n")

        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}\n")

if __name__ == "__main__":

    interactive_data_analyst()