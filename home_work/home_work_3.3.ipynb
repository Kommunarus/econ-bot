{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T14:11:31.993920121Z",
     "start_time": "2026-01-27T14:11:28.520305234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores.in_memory import InMemoryVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import os\n",
    "import re\n",
    "\n",
    "# загружаем документы\n",
    "def load_doc(listpath):\n",
    "    docs = []\n",
    "    for path in listpath:\n",
    "        name, ext = os.path.splitext(path)\n",
    "        if ext == '.pdf':\n",
    "            loader = PyPDFLoader(\n",
    "                file_path=path,\n",
    "                mode=\"page\",  # \"page\" - по страницам (по умолчанию), \"single\" - весь документ\n",
    "                extract_images=False,  # извлекать изображения\n",
    "            )\n",
    "        elif ext == '.docx':\n",
    "            loader = Docx2txtLoader(path)\n",
    "        else:\n",
    "            print('Формат не поддерживается.')\n",
    "            continue\n",
    "\n",
    "\n",
    "        doc = loader.load()\n",
    "        docs.append(doc)\n",
    "\n",
    "    return docs\n",
    "\n",
    "# удаляем текст по шаблонам, а также перетягиваем страницы на уровень вверх, они становятся как документы.\n",
    "def normaliz_cleanir_text(docs):\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        for page in doc:\n",
    "            text = page.page_content\n",
    "            text = text.lower()\n",
    "            text = re.sub(r\"^[\f]\\d+\\s*\", \"\", text, flags=re.MULTILINE)   # удаление номеров страниц.\n",
    "            text = re.sub(r\"\\s+\", \" \", text)   # свертка пробелов\n",
    "            text = re.sub(r\"(\\w)\\s(\\w)-\\n?\", r\"\\1\\2\", text)   # удаление переносов, где есть пробел (видимо особенность пдф)\n",
    "            text = re.sub(r\"(\\w)-\\n?\", r\"\\1\", text)   # удаление переносов обычного\n",
    "            text = re.sub(r\"\\n\", \" \", text)\n",
    "            text = re.sub(r\"[…]+\", \"\", text)   # удаление группы точек в содержание\n",
    "            d = text.encode('utf-8', 'ignore').decode('utf-8').strip()\n",
    "            n_d = Document(page_content=d, metadata=page.metadata)\n",
    "            new_docs.append(n_d)\n",
    "    return new_docs\n",
    "\n",
    "# делим куски на еще более мелкие чанки\n",
    "def get_chunks(docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "    splitted_docs = text_splitter.split_documents(docs)\n",
    "    return splitted_docs\n",
    "\n",
    "# техническая функция вычисления похожести двух векторов\n",
    "def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "\n",
    "# удаление дублей по хешу и по эмбеддингам. также удаление коротких кусков\n",
    "def filter_and_dedup(docs, embedding_model, min_length=30, threshold_embedding=0.95):\n",
    "    unique_hashes = set()\n",
    "    embeddings = []\n",
    "\n",
    "    filtered = []\n",
    "    stats = {'duplicates_hash': 0, 'too_short': 0, 'duplicates_embedding': 0}\n",
    "    for doc in docs:\n",
    "        text = doc.page_content.strip()\n",
    "        # короткие удаляем\n",
    "        if len(text) < min_length:\n",
    "            stats['too_short'] += 1\n",
    "            continue\n",
    "\n",
    "        # быстрый способ на равенство строк. чем длиннее строки тем быстрее хешем\n",
    "        h = hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "        if h in unique_hashes:\n",
    "            stats['duplicates_hash'] += 1\n",
    "            continue\n",
    "\n",
    "        # получаем эмбеддинги и сравниваем на похожесть\n",
    "        emb = embedding_model.embed_documents([text])[0]\n",
    "\n",
    "        if len(embeddings) > 0:\n",
    "            sims = [cosine_similarity(np.array(emb), np.array(e)) for e in embeddings]\n",
    "            max_sim = max(sims)\n",
    "            if max_sim < threshold_embedding:\n",
    "                embeddings.append(emb)\n",
    "            else:\n",
    "                stats['duplicates_embedding'] += 1\n",
    "                continue\n",
    "        else:\n",
    "            embeddings.append(emb)\n",
    "\n",
    "\n",
    "        unique_hashes.add(h)\n",
    "        filtered.append(doc)\n",
    "\n",
    "    print(f\"Первоначально: {len(docs)} чанков\")\n",
    "    print(f\"Удалено дубликатов хеш: {stats['duplicates_hash']}, слишком коротких: \\\n",
    "               {stats['too_short']}, Удалено дубликатов эмбеддинги: {stats['duplicates_embedding']}\")\n",
    "    print(f\"Осталось: {len(filtered)} чанков\")\n",
    "    return filtered"
   ],
   "id": "50117922127f48f1",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T14:12:11.507226827Z",
     "start_time": "2026-01-27T14:11:47.410137059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# специально указал один дубликат.\n",
    "embed_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "path_doc=[\"docs/Теневые сады.pdf\", 'docs/book.docx', \"docs/Теневые сады.pdf\"]\n",
    "\n",
    "raw_docs = load_doc(path_doc)\n",
    "clean_docs = normaliz_cleanir_text(raw_docs)\n",
    "chunks = get_chunks(clean_docs)\n",
    "good_docs = filter_and_dedup(chunks, embed_model, min_length=40, threshold_embedding=0.95)\n",
    "\n",
    "print('-'*50)\n",
    "print('Всего документов:', len(raw_docs), 'Всего страниц в документах:', len(clean_docs),\n",
    "      'Всего чанков:', len(chunks), 'Всего чанков после удаления дубликатов:', len(good_docs), sep='\\n')"
   ],
   "id": "9318fa47744abb97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первоначально: 1626 чанков\n",
      "Удалено дубликатов хеш: 452, слишком коротких:                6, Удалено дубликатов эмбеддинги: 6\n",
      "Осталось: 1162 чанков\n",
      "--------------------------------------------------\n",
      "Всего документов:\n",
      "3\n",
      "Всего страниц в документах:\n",
      "287\n",
      "Всего чанков:\n",
      "1626\n",
      "Всего чанков после удаления дубликатов:\n",
      "1162\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1f1d8d69fcc0b16d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
